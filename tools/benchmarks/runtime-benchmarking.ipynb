{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime Benchmarking\n",
    "\n",
    "1. Setup and build the LLRS project.\n",
    "2. Configure the benchmark by modifying the `config/runtime-benchmarking/config.yml` file.\n",
    "3. Run the following notebook to generate the benchmark results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "pd.set_option('display.max_columns', None)\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import datetime\n",
    "import subprocess\n",
    "import math\n",
    "\n",
    "project_dir = Path().resolve().parents[1]\n",
    "sys.path.insert(0, str(project_dir) + \"/modules/runtime-benchmarking/python/controllers\")\n",
    "sys.path.insert(0, str(project_dir) + \"/modules/runtime-benchmarking/python\")\n",
    "\n",
    "from operational_benchmarking_problem import OperationalBenchmarkingProblem\n",
    "import benchmark_helpers as bh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Configure Paths ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file      = str(project_dir) + \"/configs/runtime-benchmarking/config.yml\"\n",
    "problems_folder  = str(project_dir) + \"/resources/runtime-benchmark-problems\" \n",
    "solutions_folder = str(project_dir) + \"/resources/runtime-benchmark-solutions\" \n",
    "runtime_folder   = str(project_dir) + \"/resources/runtime-benchmark-data\" \n",
    "metrics_folder   = str(project_dir) + \"/resources/runtime-benchmark-metrics\" \n",
    "aod_ops_folder   = str(project_dir) + \"/resources/runtime-benchmark-aod-ops\"\n",
    "executable_path  = str(project_dir) + \"/bin/modules/runtime-benchmarking/run-bench\" \n",
    "solver_wrapper_so_file = str(project_dir) + \"/bin/modules/llrs-lib/modules/solver/libllrs-lib-solver-so.so\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folders if they do not exist\n",
    "os.makedirs(problems_folder, exist_ok=True)\n",
    "os.makedirs(solutions_folder, exist_ok=True)\n",
    "os.makedirs(runtime_folder, exist_ok=True)\n",
    "os.makedirs(metrics_folder, exist_ok=True)\n",
    "os.makedirs(aod_ops_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup logging\n",
    "logging.basicConfig(filename=str(project_dir) + '/resources/logs/benchmark-log.txt', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Define The Reconfiguration problems from <span style=\"color:orange\">**benchmark_params.yml**</span> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-defined params\n",
    "with open(config_file, 'r') as file:\n",
    "    benchmark_params = yaml.safe_load(file)\n",
    "\n",
    "#----------VARIABLES---------- \n",
    "# benchmark settings\n",
    "trial_selector    = -1\n",
    "num_trials        = benchmark_params[\"num_trials_per_problem\"]\n",
    "num_repetitions   = benchmark_params[\"repetitions_per_trial\"]\n",
    "allow_deficit     = False \n",
    "loss              = True\n",
    "seed              = 0\n",
    "if \"trial_selector\" in benchmark_params.keys():\n",
    "    trial_selector = benchmark_params[\"trial_selector\"]\n",
    "# problem definition variables default\n",
    "problem_def_dict = benchmark_params['problem_definition']\n",
    "\n",
    "\n",
    "# add default loss atom params to dict\n",
    "if \"loss_atom_params\" in problem_def_dict:\n",
    "    bh.augment_dict(problem_def_dict[\"loss_atom_params\"], bh.default_loss_atom_params)\n",
    "else:\n",
    "    problem_def_dict[\"loss_atom_params\"] = bh.default_loss_atom_params\n",
    "\n",
    "# add restricted loss env params to dict\n",
    "if \"loss_env_params\" in problem_def_dict:\n",
    "    bh.augment_dict(problem_def_dict[\"loss_env_params\"], bh.default_loss_env_params)\n",
    "else:\n",
    "    problem_def_dict[\"loss_env_params\"] = bh.default_loss_env_params\n",
    "\n",
    "# add geometry params to dict\n",
    "if \"geometry_params\" in problem_def_dict:\n",
    "    bh.augment_dict(problem_def_dict[\"geometry_params\"], bh.default_geometry_params)\n",
    "else:\n",
    "    problem_def_dict[\"geometry_params\"] = bh.default_geometry_params\n",
    "\n",
    "# problem range definition\n",
    "enable_problem_range = False\n",
    "#-----------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Create problem definition dictionaries and objects by iterating through problem_range entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define number of problems \n",
    "if(enable_problem_range):\n",
    "    # get first set of indp variables and set num_problems to its length\n",
    "    first_var_list = list(list(problem_range_dict.values())[0].values())[0]\n",
    "    num_problems = len(first_var_list)\n",
    "else:\n",
    "    # if range is not enabled, set problem range to only be the base problem \n",
    "    problem_range_dict = {\"problem_params\" : {\"Nt_y\" : [problem_def_dict[\"problem_params\"][\"Nt_y\"]]}} \n",
    "    num_problems = 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - All problems are index by a UUID, stored in output_data/problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate list of reconfiguration problems and corresponding problem def dicts\n",
    "problem_def_dict_list = []\n",
    "reconfig_problems = []\n",
    "problem_uuids = []\n",
    "\n",
    "# iterate over problem range\n",
    "for pblm_idx in range(num_problems):\n",
    "    print(f\"creating problem {pblm_idx}\")\n",
    "    np.random.seed(seed)\n",
    "    # change indp vars in new problem instance, add uuid and psf/coef path\n",
    "    temp_problem_dict = bh.augment_problem_dict(problem_def_dict, problem_range_dict, pblm_idx)\n",
    "\n",
    "    # generate problem object (with random initial configs)\n",
    "    # TODO: put this line inside the operational benchmarking block instead of here\n",
    "    temp_problem = bh.make_problem_object(temp_problem_dict, num_trials, allow_deficit, trial_selector)\n",
    "\n",
    "    problem_uuids.append(temp_problem_dict[\"uuid\"])\n",
    "    \n",
    "    # append objects and dicts to respective lists\n",
    "    problem_def_dict_list.append(temp_problem_dict)\n",
    "    reconfig_problems.append(temp_problem)\n",
    "    \n",
    "# TODO log this \n",
    "bh.save_full_problem_definitions(problem_def_dict_list, benchmark_params, reconfig_problems, problems_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Pre-solve all the problems and save the solutions of configurations over all trials/repetitions/cycles ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_obp_sols = []\n",
    "obp_metrics_arr = []\n",
    "obp_aod_ops_arr = []\n",
    "\n",
    "# for every problem, run a Monte Carlo simulation on each trial\n",
    "for pblm_idx, (rp, problem_dict) in enumerate(zip(reconfig_problems, problem_def_dict_list)):\n",
    "    print(\"_____________________________________\")\n",
    "    trial_sols = []\n",
    "    trial_metrics = []\n",
    "    trial_aod_ops = []\n",
    "    loss_env_params = problem_def_dict[\"loss_env_params\"]\n",
    "    algorithm = problem_dict[\"problem_params\"][\"algorithm\"]\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    seeds = np.random.randint(low=0, high=100*len(rp), size=len(rp))\n",
    "\n",
    "    #IN PROGRESS: inplace StaticArray creation\n",
    "    #target_static_array = \n",
    "\n",
    "    for seed_i, trial in zip(seeds, rp):\n",
    "        np.random.seed(seed_i)\n",
    "\n",
    "        # initialize OBP\n",
    "        trial_obp = OperationalBenchmarkingProblem(trial[\"initial\"], trial[\"target\"],\n",
    "                                            num_repetitions, algorithm) \n",
    "\n",
    "        # use .cu to solve and then simulate loss, num_repetitions times\n",
    "        trial_obp_sol, trial_obp_metrics, trial_obp_aod_ops = trial_obp.pre_solve(loss, \n",
    "                                                                                  loss_env_params[\"t_alpha\"], \n",
    "                                                                                  loss_env_params[\"t_nu\"], \n",
    "                                                                                  loss_env_params[\"t_latency\"],\n",
    "                                                                                  solver_wrapper_so_file) \n",
    "\n",
    "        trial_sols.append(trial_obp_sol)\n",
    "        trial_metrics.append(trial_obp_metrics)\n",
    "        trial_aod_ops.append(trial_obp_aod_ops)\n",
    "    \n",
    "    problem_obp_sols.append(trial_sols)\n",
    "    obp_metrics_arr.append(trial_metrics)\n",
    "    obp_aod_ops_arr.append(trial_aod_ops)\n",
    "    print(\"running problem {}\".format(pblm_idx))\n",
    "\n",
    "# save solutions to \"uuid.json\" in solutions folder. Also used by runtime benchmarking.\n",
    "bh.save_obp_sols(problem_def_dict_list, problem_obp_sols, solutions_folder)\n",
    "bh.save_obp_sols(problem_def_dict_list, obp_metrics_arr, metrics_folder)\n",
    "bh.save_obp_sols(problem_def_dict_list, obp_aod_ops_arr, aod_ops_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Run all problems and <span style=\"color:orange\">**collect run-time data**</span> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_exp_params = problem_def_dict_list[0]['experiment_params']\n",
    "\n",
    "iteration_object_with_progress_bar = tqdm(problem_def_dict_list, \n",
    "                                          desc='Processing problems', \n",
    "                                          bar_format='{l_bar}{bar:3}{r_bar}',\n",
    "                                          total=len(problem_def_dict_list))\n",
    "\n",
    "# execute C with all different saved json files\n",
    "\n",
    "subprocess.run([\"cp\", config_file, str(project_dir) + \"/configs/llrs/runtime-temp.yml\"])\n",
    "for problem_def_dict in iteration_object_with_progress_bar:\n",
    "    command = executable_path + \" runtime-temp.yml \" + problem_def_dict['uuid']\n",
    "    print(\"running\", command)\n",
    "    os.system(command)\n",
    "subprocess.run([\"rm\", str(project_dir) + \"/configs/llrs/runtime-temp.yml\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Format each problem solutions with its benchmark results as a dataframe, store in desired path ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_2D = bh.is_2D_problem(problem_def_dict)\n",
    "cur_time = str(datetime.datetime.now())\n",
    "base_path = str(project_dir) + \"/resources/pickled_results/\" + cur_time.replace(\" \", \"/\").replace(\":\", \"-\").replace(\".\", \"-\")\n",
    "os.makedirs(base_path, exist_ok = True)\n",
    "cur_time = int(time.time())\n",
    "for puuid in problem_uuids:\n",
    "    formatted_data_path = f\"{base_path}/{puuid}.pickle\"\n",
    "    bh.store_formatted_benchmark_data(\n",
    "        experiment_id=puuid, \n",
    "        runtime_dir=runtime_folder, \n",
    "        metrics_dir=metrics_folder, \n",
    "        config_dir=solutions_folder, \n",
    "        aod_ops_dir=aod_ops_folder, \n",
    "        problem_def_dir=problems_folder, \n",
    "        output_path=formatted_data_path, \n",
    "        is_2D=is_2D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        prob = pickle.load(f)\n",
    "\n",
    "        print(prob[\"data\"].columns.values)\n",
    "\n",
    "        null_value = pd.Series()\n",
    "        # we should probably make the settings.h into a yaml so we don't have to hard code it here\n",
    "        waveform_duration = 10e4\n",
    "        # waveforms streamed in step IV (instead of V)\n",
    "        prob[\"data\"][\"Total_Waveforms\"] = ((prob[\"data\"][prob[\"data\"][\"Cycle\"] > 0])[\"num_nu_operations\"] + \\\n",
    "                                          (prob[\"data\"][prob[\"data\"][\"Cycle\"] > 0])[\"num_alpha_operations\"]).mean()\n",
    "        prob[\"data\"][\"S-IP_Total\"] = prob[\"data\"].get(\"II-Deconvolution\", null_value) + prob[\"data\"].get(\"II-Threshold\", null_value)\n",
    "        prob[\"data\"][\"S-Pre-lookup\"] = prob[\"data\"].get(\"V-First-Lookup\", null_value)\n",
    "        prob[\"data\"][\"S-Pre-upload\"] = prob[\"data\"].get(\"V-First-Upload\", null_value) + prob[\"data\"].get(\"V-Second-Upload\", null_value)\n",
    "        prob[\"data\"][\"S-Pre-update\"] = prob[\"data\"].get(\"V-First-Update\", null_value)\n",
    "        prob[\"data\"][\"S-Latency_Total\"] = prob[\"data\"][\"S-Pre-lookup\"] + prob[\"data\"][\"S-Pre-upload\"] + prob[\"data\"][\"S-Pre-update\"]\n",
    "        prob[\"data\"][\"S-Synth_Load_Total\"] = prob[\"data\"].get(\"IV-Translate\", null_value) + prob[\"data\"][\"S-Latency_Total\"]\n",
    "        prob[\"data\"][\"S-Conc_Load_Stream\"] = prob[\"data\"].get(\"V-Load_Stream\", null_value) - prob[\"data\"][\"S-Latency_Total\"]\n",
    "        prob[\"data\"][\"S-Theoretical_Stream\"] = prob[\"data\"][\"Total_Waveforms\"] * waveform_duration\n",
    "        prob[\"data\"][\"S-Stream_Latency\"] = prob[\"data\"][\"S-Conc_Load_Stream\"] - prob[\"data\"][\"S-Theoretical_Stream\"]\n",
    "        prob[\"data\"][\"S-Total\"] = prob[\"data\"].get(\"I\", null_value) + prob[\"data\"][\"S-IP_Total\"] + prob[\"data\"].get(\"III-Total\", null_value) + prob[\"data\"].get(\"V-Load_Stream\", null_value) + prob[\"data\"].get(\"IV-Translate\", null_value)\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Load Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(str(project_dir))\n",
    "BENCHMARKING_FIGURES_PATH = \"../../../figures/benchmarking/\"\n",
    "path_1D =  \"/home/tqtraaqs1/LLRS/resources/pickled_results/2024-04-14/20-51-34-299566/6d823d09-284a-4425-8c87-8fa8826e7a37.pickle\"\n",
    "path_2D =  \"/home/tqtraaqs1/LLRS/resources/pickled_results/2024-04-14/21-11-01-535629/0fec3a34-b1a3-4e7b-8d08-0fffd1b7abf3.pickle\"\n",
    "path_2D2 = \"/home/tqtraaqs1/LLRS/resources/pickled_results/2024-04-14/21-11-01-535629/0fec3a34-b1a3-4e7b-8d08-0fffd1b7abf3.pickle\"\n",
    "full_pickle_1D = preprocess(path_1D)\n",
    "full_pickle_1D[\"title\"] = \"Linear 1D\"\n",
    "print(\"1D id: \", full_pickle_1D['problem_definition']['uuid'])\n",
    "\n",
    "print()\n",
    "\n",
    "full_pickle_2D = preprocess(path_2D)\n",
    "full_pickle_2D[\"title\"] = \"REDREC\\_V2\"\n",
    "print(\"2D id: \", full_pickle_2D['problem_definition']['uuid'])\n",
    "\n",
    "\n",
    "full_pickle_2D2 = preprocess(path_2D2)\n",
    "full_pickle_2D2[\"title\"] = \"REDREC\\_V2\"\n",
    "print(\"2D id: \", full_pickle_2D2['problem_definition']['uuid'])\n",
    "\n",
    "# uncomment to replace runtime data with dummy data for clarity \n",
    "\n",
    "#full_pickle_1D[\"title\"] = \"FAKE\"\n",
    "#for col in full_pickle_1D[\"data\"].columns:\n",
    "#   full_pickle_1D[\"data\"][col].values[:] = 0\n",
    "\n",
    "#full_pickle_2D[\"title\"] = \"FAKE\"\n",
    "#for col in full_pickle_2D[\"data\"].columns:\n",
    "#     full_pickle_2D[\"data\"][col].values[:] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_pickle_1D.keys())\n",
    "print(full_pickle_1D['problem_definition'].keys())\n",
    "print(full_pickle_1D['problem_definition']['problem_params'].keys())\n",
    "print(full_pickle_1D['problem_definition']['experiment_params'].keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{**full_pickle_1D, 'problem_definition' : {**full_pickle_1D['problem_definition'],  'problem_params': \n",
    " {**full_pickle_1D['problem_definition']['problem_params'], \n",
    "  'target_config':str(full_pickle_1D['problem_definition']['problem_params']['target_config'])}}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{**full_pickle_2D, 'problem_definition' : {**full_pickle_2D['problem_definition'],  'problem_params': \n",
    " {**full_pickle_2D['problem_definition']['problem_params'], \n",
    "  'target_config':str(full_pickle_2D['problem_definition']['problem_params']['target_config'])}}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_pickle_1D[\"title\"])\n",
    "full_pickle_1D[\"data\"].describe().drop('count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pickle_2D[\"data\"].describe().drop('count')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print mean and median and std dev for relevant columns\n",
    "ns_per_ms = 1e6\n",
    "\n",
    "titles = [\"I. Image Acquisition\",\n",
    "          \"II.1 Deconvolution\",\n",
    "          \"II.2 Thresholding\",\n",
    "          \"III.1 Generate matching\",\n",
    "          \"III.2 Batching\",\n",
    "          \"IV.1 Generate table keys\",\n",
    "          \"IV-V.1 Pre-lookup\",\n",
    "          \"IV-V.2 Pre-upload\",\n",
    "          \"IV-V.3 Pre-update\",\n",
    "          \"IV-V.4 Waveform Synthesis & Streaming\"]\n",
    "\n",
    "stages = [\"Image Acquisition\",\n",
    "          \"Image Processing\",\n",
    "          \"Problem Solving\",\n",
    "          \"Waveform Synthesis & Streaming\"]\n",
    "\n",
    "sub_stages_list = [[\"I\"],\n",
    "              [\"II-Deconvolution\",\n",
    "               \"II-Threshold\",\n",
    "               \"S-IP_Total\"],\n",
    "              [\"III-Matching\",\n",
    "               \"III-Batching\",\n",
    "               \"III-Total\"],\n",
    "              [\"IV-Translate\",\n",
    "               \"V-First-Lookup\",\n",
    "               \"V-First-Upload\",\n",
    "               \"V-Second-Upload\",\n",
    "               \"V-First-Update\",\n",
    "               \"V-Load_Stream\",\n",
    "               \"V-Latency\",\n",
    "               \"S-Pre-lookup\",\n",
    "               \"S-Pre-upload\",\n",
    "               \"S-Pre-update\",\n",
    "               \"S-Latency_Total\",\n",
    "               \"S-Synth_Load_Total\",\n",
    "               \"S-Conc_Load_Stream\"]]\n",
    "\n",
    "def print_runtime_summary(full_pickle):\n",
    "    \n",
    "    Nt_x = full_pickle[\"problem_definition\"][\"problem_params\"][\"Nt_x\"]\n",
    "    Nt_y = full_pickle[\"problem_definition\"][\"problem_params\"][\"Nt_y\"]\n",
    "    num_target = full_pickle[\"problem_definition\"][\"problem_params\"][\"num_target\"]\n",
    "    num_traps = Nt_x * Nt_y\n",
    "    roi_height = full_pickle[\"problem_definition\"][\"experiment_params\"][\"roi_height\"]\n",
    "    roi_width = full_pickle[\"problem_definition\"][\"experiment_params\"][\"roi_width\"]\n",
    "\n",
    "    print(\"FORMAT: Component: Mean, Median(Std Dev)\")\n",
    "    print(full_pickle[\"title\"])\n",
    "    print(f\"N_a = {num_target}, N_t = {num_traps}\")\n",
    "    print(f\"ROI = {roi_height} X {roi_width}\")\n",
    "    \n",
    "    runtime_data_dict = full_pickle[\"data\"]\n",
    "    for (stage, sub_stages) in zip(stages, sub_stages_list):\n",
    "        print(f\"------{stage}------\")\n",
    "        for sub_stage in sub_stages:\n",
    "            value = runtime_data_dict.get(sub_stage, pd.Series())\n",
    "            print(f\"{sub_stage}: {value.mean()/ns_per_ms}, {value.median()/ns_per_ms}({value.std()/ns_per_ms})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def theoretical_acquistion_time_aug21(T_exposure=0.02, f_v=1/4.33e-6, f_h=30e6, f_PCIe=1.3875e9, f_link=6.1e9, S_y=1039, \n",
    "                                      N_D=468, N_G=604, N_O=16, W=1024, H=1024, S_h=1056, bin_v=1, DMA_Const = 0.289e-3):\n",
    "    print(H)\n",
    "\n",
    "    T_transfer = S_y/f_v + H/f_v * sum([1/(2**i)for i in range(1, int(np.log2(bin_v)))])\n",
    "    T_horshift = (H)/bin_v * (1/f_v + (S_h + N_O - 1024 + W)/f_h)\n",
    "    T_register = (N_D + N_G)/f_h\n",
    "    #T_link = 8 * W * H/bin_v / f_link\n",
    "    T_link = 0\n",
    "    T_dma = DMA_Const +H/bin_v * W * 2 / f_PCIe\n",
    "\n",
    "    res = {\"T_exposure\" : T_exposure * 1e3,\n",
    "           \"T_transfer\" : T_transfer * 1e3,\n",
    "           \"T_horshift\" : T_horshift * 1e3,\n",
    "           \"T_register\" : T_register * 1e3,\n",
    "           \"T_link\" : 0,#T_link,\n",
    "           \"T_dma\" : T_dma * 1e3,\n",
    "           \"T_total\" : (T_exposure + T_transfer + T_horshift + T_register + T_link+ T_dma) * 1e3}\n",
    "    print(W)\n",
    "    print(res)\n",
    "\n",
    "    return res\n",
    "\n",
    "def compile_latex(latex_code, output_path):\n",
    "    with open('temp.tex', 'w') as file:\n",
    "        file.write(latex_code)\n",
    "\n",
    "    # Compile the LaTeX code into a PDF file\n",
    "    run(['pdflatex', 'temp.tex'])\n",
    "    run(['mv', 'temp.pdf', output_path])\n",
    "\n",
    "def replace_variables_in_template(template_path, variables):\n",
    "    with open(template_path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Replace placeholders with their corresponding values\n",
    "    for key, value in variables.items():\n",
    "        content = content.replace('##' + key + '##', str(value))\n",
    "\n",
    "    return content\n",
    "\n",
    "template_path = str(project_dir) + '/modules/runtime-benchmarking/table_template.tex'\n",
    "output_filename = f\"runtime_table_{datetime.datetime.now()}.pdf\"\n",
    "output_path = f\"{BENCHMARKING_FIGURES_PATH}/{output_filename}\"\n",
    "\n",
    "IA_pred_dict_1D = theoretical_acquistion_time_aug21(W=full_pickle_1D[\"problem_definition\"][\"experiment_params\"][\"roi_width\"], H=full_pickle_1D[\"problem_definition\"][\"experiment_params\"][\"roi_height\"])\n",
    "IA_pred_dict_2D = theoretical_acquistion_time_aug21(W=full_pickle_2D[\"problem_definition\"][\"experiment_params\"][\"roi_width\"] ,H=full_pickle_2D[\"problem_definition\"][\"experiment_params\"][\"roi_height\"])\n",
    "IA_pred_dict_2D2 = theoretical_acquistion_time_aug21(W=full_pickle_2D2[\"problem_definition\"][\"experiment_params\"][\"roi_width\"] ,H=full_pickle_2D2[\"problem_definition\"][\"experiment_params\"][\"roi_height\"])\n",
    "\n",
    "#print(IA_pred_dict_2D[\"T_horshift\"], IA_pred_dict_2D[\"T_register\"])\n",
    "\n",
    "#print(full_pickle_1D[\"problem_definition\"][\"problem_params\"][\"Nt_x\"])\n",
    "#print(full_pickle_2D[\"problem_definition\"][\"problem_params\"][\"Nt_x\"])\n",
    "\n",
    "# Variables to be inserted into the LaTeX template\n",
    "variables = {\n",
    "    \"1D_path\" : path_1D,\n",
    "    \"1D_title\": full_pickle_1D[\"title\"],\n",
    "    \"1D_Nt_x\": full_pickle_1D[\"problem_definition\"][\"problem_params\"][\"Nt_x\"],\n",
    "    \"1D_Nt_y\" : full_pickle_1D[\"problem_definition\"][\"problem_params\"][\"Nt_y\"],\n",
    "    \"1D_N_a1\" : int(math.sqrt(full_pickle_1D[\"problem_definition\"][\"problem_params\"][\"num_target\"])),\n",
    "    \"1D_N_a2\" : int(math.sqrt(full_pickle_1D[\"problem_definition\"][\"problem_params\"][\"num_target\"])),\n",
    "    \"1D_Nt1\" : int(math.sqrt(full_pickle_1D[\"problem_definition\"][\"problem_params\"][\"num_target\"])),\n",
    "    \"1D_Nt2\" : int((full_pickle_1D[\"problem_definition\"][\"problem_params\"][\"Nt_x\"] * \n",
    "                full_pickle_1D[\"problem_definition\"][\"problem_params\"][\"Nt_y\"]) // \n",
    "                math.sqrt(full_pickle_1D[\"problem_definition\"][\"problem_params\"][\"num_target\"])),\n",
    "    \"1D_roi_height\" : full_pickle_1D[\"problem_definition\"][\"experiment_params\"][\"roi_height\"],\n",
    "    \"1D_roi_width\" : full_pickle_1D[\"problem_definition\"][\"experiment_params\"][\"roi_width\"],\n",
    "    \"1D_IA_total_avg\" : \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"I\"].mean()/ns_per_ms, 3)),\n",
    "    \"1D_IA_total_std\" : \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"I\"].std()/ns_per_ms, 3)),\n",
    "    \"1D_IA_exposure\" : \"{:.3f}\".format(round(IA_pred_dict_1D[\"T_exposure\"])),\n",
    "    \"1D_IA_frame_transfer\" : \"{:.3f}\".format(round(IA_pred_dict_1D[\"T_transfer\"], 3)),\n",
    "    \"1D_IA_horz_shift\" : \"{:.3f}\".format(round(IA_pred_dict_1D[\"T_horshift\"] + IA_pred_dict_1D[\"T_register\"], 3)),\n",
    "    \"1D_IA_DMA_transfer\" : \"{:.3f}\".format(round(IA_pred_dict_1D[\"T_dma\"] + full_pickle_1D[\"data\"][\"I\"].mean()/ns_per_ms - IA_pred_dict_1D[\"T_total\"], 3)),\n",
    "    \"1D_IA_latency\" : \"{:.3f}\".format(round((full_pickle_1D[\"data\"][\"I\"].mean()/ns_per_ms) - IA_pred_dict_1D[\"T_total\"], 3)),\n",
    "    \"1D_IP_total_avg\" : \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"S-IP_Total\"].mean()/ns_per_ms, 3)),\n",
    "    \"1D_IP_total_std\" : \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"S-IP_Total\"].std()/ns_per_ms, 3)),\n",
    "    \"1D_IP_deconvolution\" : \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"II-Deconvolution\"].mean()/ns_per_ms, 3)),\n",
    "    \"1D_IP_threshold\" : \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"II-Threshold\"].mean()/ns_per_ms, 3)),\n",
    "    \"1D_PS_total_avg\" : \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"III-Total\"].mean()/ns_per_ms, 3)),\n",
    "    \"1D_PS_total_std\" : \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"III-Total\"].std()/ns_per_ms, 3)),\n",
    "    \"1D_PS_matching\" : \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"III-Matching\"].mean()/ns_per_ms, 3)),\n",
    "    \"1D_PS_batching\" : \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"III-Batching\"].mean()/ns_per_ms, 3)),\n",
    "    \"1D_SS_total_avg\": \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"S-Synth_Load_Total\"].mean()/ns_per_ms, 3)),\n",
    "    \"1D_SS_total_std\": \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"S-Synth_Load_Total\"].std()/ns_per_ms, 3)),\n",
    "    \"1D_SS_translate\": \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"IV-Translate\"].mean()/ns_per_ms, 3)),\n",
    "    \"1D_SS_Pre-lookup\" : \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"S-Pre-lookup\"].mean()/ns_per_ms, 3)),\n",
    "    \"1D_SS_Pre-upload\" : \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"S-Pre-upload\"].mean()/ns_per_ms, 3)),\n",
    "    \"1D_SS_Pre-update\" : \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"S-Pre-update\"].mean()/ns_per_ms, 3)),\n",
    "    \"1D_SS_Conc_Load_Stream\" : \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"S-Conc_Load_Stream\"].mean()/ns_per_ms, 3)),\n",
    "    \"1D_SS_Conc_Load_Stream_std\" : \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"S-Conc_Load_Stream\"].std()/ns_per_ms, 3)),\n",
    "    \"1D_SS_Theoretical_Stream\" : \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"S-Theoretical_Stream\"].mean()/ns_per_ms, 3)),\n",
    "    \"1D_SS_Stream_Latency\" : \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"S-Stream_Latency\"].mean()/ns_per_ms, 3)),\n",
    "    \"1D_SS_Stream_Latency_std\" : \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"S-Stream_Latency\"].std()/ns_per_ms, 3)),\n",
    "    \"1D_overall_avg\" : \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"S-Total\"].mean()/ns_per_ms, 3)),\n",
    "    \"1D_overall_std\" : \"{:.3f}\".format(round(full_pickle_1D[\"data\"][\"S-Total\"].std()/ns_per_ms, 3)),\n",
    "    \"2D_path\" : path_2D,\n",
    "    \"2D_title\": full_pickle_2D[\"title\"],\n",
    "    \"2D_Nt_x\": full_pickle_2D[\"problem_definition\"][\"problem_params\"][\"Nt_x\"],\n",
    "    \"2D_Nt_y\" : full_pickle_2D[\"problem_definition\"][\"problem_params\"][\"Nt_y\"],\n",
    "    \"2D_N_a1\" : int(math.sqrt(full_pickle_2D[\"problem_definition\"][\"problem_params\"][\"num_target\"])),\n",
    "    \"2D_N_a2\" : int(math.sqrt(full_pickle_2D[\"problem_definition\"][\"problem_params\"][\"num_target\"])),\n",
    "    \"2D_Nt1\" : int(math.sqrt(full_pickle_2D[\"problem_definition\"][\"problem_params\"][\"num_target\"])),\n",
    "    \"2D_Nt2\" : int((full_pickle_2D[\"problem_definition\"][\"problem_params\"][\"Nt_x\"] * \n",
    "                full_pickle_2D[\"problem_definition\"][\"problem_params\"][\"Nt_y\"]) // \n",
    "                math.sqrt(full_pickle_2D[\"problem_definition\"][\"problem_params\"][\"num_target\"])),\n",
    "    \"2D_roi_height\" : full_pickle_2D[\"problem_definition\"][\"experiment_params\"][\"roi_height\"],\n",
    "    \"2D_roi_width\" : full_pickle_2D[\"problem_definition\"][\"experiment_params\"][\"roi_width\"],\n",
    "    \"2D_IA_total_avg\" : \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"I\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_IA_total_std\" : \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"I\"].std()/ns_per_ms, 3)),\n",
    "    \"2D_IA_exposure\" : \"{:.3f}\".format(round(IA_pred_dict_2D[\"T_exposure\"])),\n",
    "    \"2D_IA_frame_transfer\" : \"{:.3f}\".format(round(IA_pred_dict_2D[\"T_transfer\"], 3)),\n",
    "    \"2D_IA_horz_shift\" : \"{:.3f}\".format(round(IA_pred_dict_2D[\"T_horshift\"] + IA_pred_dict_2D[\"T_register\"], 3)),\n",
    "    \"2D_IA_DMA_transfer\" : \"{:.3f}\".format(round(IA_pred_dict_2D[\"T_dma\"] + full_pickle_2D[\"data\"][\"I\"].mean()/ns_per_ms - IA_pred_dict_2D[\"T_total\"], 3)),\n",
    "    \"2D_IA_latency\" : \"{:.3f}\".format(round((full_pickle_2D[\"data\"][\"I\"].mean()/ns_per_ms) - IA_pred_dict_2D[\"T_total\"], 3)),\n",
    "    \"2D_IP_total_avg\" : \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"S-IP_Total\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_IP_total_std\" : \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"S-IP_Total\"].std()/ns_per_ms, 3)),\n",
    "    \"2D_IP_deconvolution\" : \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"II-Deconvolution\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_IP_threshold\" : \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"II-Threshold\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_PS_total_avg\" : \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"III-Total\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_PS_total_std\" : \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"III-Total\"].std()/ns_per_ms, 3)),\n",
    "    \"2D_PS_matching\" : \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"III-Matching\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_PS_batching\" : \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"III-Batching\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_SS_total_avg\": \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"S-Synth_Load_Total\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_SS_total_std\": \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"S-Synth_Load_Total\"].std()/ns_per_ms, 3)),\n",
    "    \"2D_SS_translate\": \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"IV-Translate\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_SS_Pre-lookup\" : \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"S-Pre-lookup\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_SS_Pre-upload\" : \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"S-Pre-upload\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_SS_Pre-update\" : \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"S-Pre-update\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_SS_Conc_Load_Stream\" : \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"S-Conc_Load_Stream\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_SS_Conc_Load_Stream_std\" : \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"S-Conc_Load_Stream\"].std()/ns_per_ms, 3)),\n",
    "    \"2D_SS_Theoretical_Stream\" : \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"S-Theoretical_Stream\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_SS_Stream_Latency\" : \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"S-Stream_Latency\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_SS_Stream_Latency_std\" : \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"S-Stream_Latency\"].std()/ns_per_ms, 3)),\n",
    "    \"2D_overall_avg\" : \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"S-Total\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_overall_std\" : \"{:.3f}\".format(round(full_pickle_2D[\"data\"][\"S-Total\"].std()/ns_per_ms, 3)),\n",
    "    \"2D_path2\" : path_2D2,\n",
    "    \"2D_title2\": full_pickle_2D2[\"title\"],\n",
    "    \"2D_Nt_x2\": full_pickle_2D2[\"problem_definition\"][\"problem_params\"][\"Nt_x\"],\n",
    "    \"2D_Nt_y2\" : full_pickle_2D2[\"problem_definition\"][\"problem_params\"][\"Nt_y\"],\n",
    "    \"2D_N_a12\" : int(math.sqrt(full_pickle_2D2[\"problem_definition\"][\"problem_params\"][\"num_target\"])),\n",
    "    \"2D_N_a22\" : int(math.sqrt(full_pickle_2D2[\"problem_definition\"][\"problem_params\"][\"num_target\"])),\n",
    "    \"2D_Nt12\" : int(math.sqrt(full_pickle_2D2[\"problem_definition\"][\"problem_params\"][\"num_target\"])),\n",
    "    \"2D_Nt22\" : int((full_pickle_2D2[\"problem_definition\"][\"problem_params\"][\"Nt_x\"] * \n",
    "                full_pickle_2D2[\"problem_definition\"][\"problem_params\"][\"Nt_y\"]) // \n",
    "                math.sqrt(full_pickle_2D2[\"problem_definition\"][\"problem_params\"][\"num_target\"])),\n",
    "    \"2D_roi_height2\" : full_pickle_2D2[\"problem_definition\"][\"experiment_params\"][\"roi_height\"],\n",
    "    \"2D_roi_width2\" : full_pickle_2D2[\"problem_definition\"][\"experiment_params\"][\"roi_width\"],\n",
    "    \"2D_n_alpha2\" : 1,\n",
    "    \"2D_n_nu2\" : 1,\n",
    "    \"2D_IA_total_avg2\" : \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"I\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_IA_total_std2\" : \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"I\"].std()/ns_per_ms, 3)),\n",
    "    \"2D_IA_exposure2\" : \"{:.3f}\".format(round(IA_pred_dict_2D2[\"T_exposure\"])),\n",
    "    \"2D_IA_frame_transfer2\" : \"{:.3f}\".format(round(IA_pred_dict_2D2[\"T_transfer\"], 3)),\n",
    "    \"2D_IA_horz_shift2\" : \"{:.3f}\".format(round(IA_pred_dict_2D2[\"T_horshift\"] + IA_pred_dict_2D2[\"T_register\"], 3)),\n",
    "    \"2D_IA_DMA_transfer2\" : \"{:.3f}\".format(round(IA_pred_dict_2D2[\"T_dma\"] + full_pickle_2D2[\"data\"][\"I\"].mean()/ns_per_ms - IA_pred_dict_2D2[\"T_total\"], 3)),\n",
    "    \"2D_IA_latency2\" : \"{:.3f}\".format(round((full_pickle_2D2[\"data\"][\"I\"].mean()/ns_per_ms) - IA_pred_dict_2D2[\"T_total\"], 3)),\n",
    "    \"2D_IP_total_avg2\" : \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"S-IP_Total\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_IP_total_std2\" : \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"S-IP_Total\"].std()/ns_per_ms, 3)),\n",
    "    \"2D_IP_deconvolution2\" : \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"II-Deconvolution\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_IP_threshold2\" : \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"II-Threshold\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_PS_total_avg2\" : \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"III-Total\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_PS_total_std2\" : \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"III-Total\"].std()/ns_per_ms, 3)),\n",
    "    \"2D_PS_matching2\" : \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"III-Matching\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_PS_batching2\" : \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"III-Batching\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_SS_total_avg2\": \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"S-Synth_Load_Total\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_SS_total_std2\": \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"S-Synth_Load_Total\"].std()/ns_per_ms, 3)),\n",
    "    \"2D_SS_translate2\": \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"IV-Translate\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_SS_Pre-lookup2\" : \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"S-Pre-lookup\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_SS_Pre-upload2\" : \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"S-Pre-upload\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_SS_Pre-update2\" : \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"S-Pre-update\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_SS_Conc_Load_Stream2\" : \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"S-Conc_Load_Stream\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_SS_Conc_Load_Stream_std2\" : \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"S-Conc_Load_Stream\"].std()/ns_per_ms, 3)),\n",
    "    \"2D_SS_Theoretical_Stream2\" : \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"S-Theoretical_Stream\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_SS_Stream_Latency2\" : \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"S-Stream_Latency\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_SS_Stream_Latency_std2\" : \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"S-Stream_Latency\"].std()/ns_per_ms, 3)),\n",
    "    \"2D_overall_avg2\" : \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"S-Total\"].mean()/ns_per_ms, 3)),\n",
    "    \"2D_overall_std2\" : \"{:.3f}\".format(round(full_pickle_2D2[\"data\"][\"S-Total\"].std()/ns_per_ms, 3))\n",
    "}\n",
    "\n",
    "\n",
    "# Replace placeholders with actual values\n",
    "updated_latex_code = replace_variables_in_template(template_path, variables)\n",
    "\n",
    "# Compile the updated LaTeX code into a PDF file\n",
    "compile_latex(updated_latex_code, output_path)\n",
    "print('Result compiled to', output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(IA_pred_dict_2D)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tqtraaqs3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
