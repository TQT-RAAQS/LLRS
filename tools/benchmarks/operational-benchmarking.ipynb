{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operational Benchmarking\n",
    "\n",
    "1. Setup and build the LLRS project.\n",
    "2. Configure the benchmark by modifying the `config/operational-benchmarking/config.yml` file.\n",
    "3. Run the following notebook to generate the benchmark results.\n",
    "\n",
    "- The production-quality figure generation of this notebook uses the `Experiment` module's Python package, make sure to install it. (The guide to install the package is available in the `README.md` file of the `Experiment` module.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import yaml\n",
    "import pickle\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "project_dir = Path().resolve().parents[1]\n",
    "sys.path.insert(0, project_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = str(project_dir) + \"/bin/modules/operational-benchmarking/op-bench\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the config file for the benchmark. The default value is `config/benchmark_params_cpp.yml`, the path given must be relative to the `LLRS` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = str(project_dir) + \"/configs/operational-benchmarking/config.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-defined params\n",
    "with open(config_path, 'r') as file:\n",
    "    benchmark_params = yaml.safe_load(file)\n",
    "\n",
    "benchmark_names = []\n",
    "benchmarks = []\n",
    "for it in benchmark_params[\"range_param\"]:\n",
    "    benchmark_names.append(\"benchmark_\" + str(it))\n",
    "    benchmarks.append([benchmark_params[\"num_trials\"], benchmark_params[\"num_repetition\"], it])\n",
    "    print(f\"Creating: {it}\")\n",
    "\n",
    "variable_param = benchmark_params[\"variable_param\"]\n",
    "\n",
    "# create runtime directories\n",
    "resources_dir = str(project_dir) + \"resources/runtime/operational_benchmarking/\" + str(time.time())\n",
    "Path(resources_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# create the temp config file\n",
    "with open(config_path, 'r') as file:\n",
    "    config_str = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for it in benchmarks:\n",
    "    created_config = resources_dir + \"/config.yml\"\n",
    "    with open(created_config, \"w\") as file:\n",
    "        file.write(config_str.format(it[2]))\n",
    "    result = subprocess.Popen([executable_path, created_config, str(it[1]), str(it[2])], stdout=subprocess.PIPE, text=True)\n",
    "    result.wait()\n",
    "    # Display the output\n",
    "    raw_output = result.stdout.read().strip().replace('\\r\\n', '\\n').replace('\\r', '')\n",
    "    data.append(float(raw_output))\n",
    "    print(\"Processed C++ Output:\", data[-1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Operational Benchmarking S-Shape Curve ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import experiment.toolkits.figure_formatting.formatter_2023_az as fmt\n",
    "import experiment.toolkits.figure_formatting.production_quality_plots as prod_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = benchmark_params[\"range_param\"]\n",
    "fig, ax = plt.subplots() \n",
    "ax.set_xlabel(benchmark_params[variable_param])\n",
    "ax.set_ylabel(\"Success Rate (p)\")\n",
    "\n",
    "if min(x_list) == max(x_list):\n",
    "    ax.set_xlim(min(x_list) - 1, max(x_list)+ 1)\n",
    "else:   \n",
    "    ax.set_xlim(min(x_list), max(x_list))\n",
    "ax.set_ylim(0,1)\n",
    "\n",
    "\n",
    "prod_plot.production_quality_scatter(fig, ax, x_list, data, num_xticks = 3, save_path=resources_dir + \"/figures/mean_success.pdf\")\n",
    "\n",
    "print(\"Figure saved to:\" + resources_dir + \"/figures/mean_success.pdf\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tqtraaqs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
